{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Libraries:__\n",
    "\n",
    "Given the string \"0+69\", the model should return a prediction: \"69\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense, Dropout, SimpleRNN, RepeatVector\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback\n",
    "\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generate Data:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we store characters in the memory, then we make two dictionaries one to tokenize the characters in the memory into numerics. we enumerate over char list, and set char values key and the indicies as values. Hence we get the tokenize representation of the characters.\n",
    "then we another dictionary, kind of opposite of what we made above. just that indices will be keys here and char going to be corresponding values.\n",
    "\n",
    "then next we define a function to return one example and its corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hP-OyQrivBNl",
    "outputId": "70a8ce77-3241-4ea9-dbc7-d9bebb3789f3"
   },
   "outputs": [],
   "source": [
    "all_chars = '0123456789+-*/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  14\n"
     ]
    }
   ],
   "source": [
    "num_of_features = len(all_chars)\n",
    "print(\"Number of features: \", num_of_features)\n",
    "char_to_index = dict((c, i) for i, c in enumerate(all_chars))\n",
    "index_to_char = dict((i, c) for i, c in enumerate(all_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2b1Xg9hvBNr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('29+50', '79')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_data():\n",
    "    f = np.random.randint(0, 100)\n",
    "    s = np.random.randint(0, 100)\n",
    "    exp = str(f) + '+' + str(s)\n",
    "    label = str(f+s)\n",
    "    return exp, label\n",
    "\n",
    "generate_data()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create the Model:__\n",
    "\n",
    "Consider these two reviews:\n",
    "\n",
    "Review 1: This movie is not terrible at all.\n",
    "\n",
    "Review 2: This movie is pretty decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike regular Neural Networks, RNNs allow us to input vectors of varied lengths as examples and potentially also get labels of varied lengths. Meaning if we have input of different lengths such as in sentiment analysis of twitter tweets or imbd reviews. Length of all these input sentences varied in length. so, in normal NN we need to preprocess the input in such a way that all those sentences are padded with meaningless words to make them allof same length as of the one with longest length. whereas in RNNs we don't need to do it, it gives us equally great results even with different length input sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here we using keras to create RNN, which easy and efficient of way buliding a RNN model. We going to use kears' simple RNN layer, i.e., a fully connected RNN layer and ouputs of this layer are fed back into the RNN model. This uses a tanh activation function by default and we leave it as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We using total 128 hidden layers. Two nos. in the input will be 0 to 99, so maximum length of a input expression will be 5. Can also be said as the maximum no. time steps in input sequences.\n",
    "The model we making has two section to it, first part called the encoder is single simple RNN layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create first layer, the simple RNN layer with arguments, no. of hidden layer, input shape which again has two argument, time steps and num of features defined above. And output of this layer will be a single vector representation. And to acheive this single vector representation of this layer we use repeat vector layer. In it we specify no. of times it should repeat, which is maximum no. of time steps. This makes the encoder part, and it's output will be fed to decoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in decoder, need to create another simple RNN layer. And this fill take vector representation as input and generate a predicted sequence. It will written simlar to the one above, but since we need to return the sequence we set return sequence argument to true. \n",
    "Then out of this layer will go into a dense layer with softmax activation function. Because we need possibility of various possible charaters for each time step.\n",
    "Now we using a dense layer, but we encapsulate it inside a tie disturbution layer. So that the model knows we want to apply dense layer to individual time steps and hidden state is different for different time steps.\n",
    "So we make a time distribution layer inside it we write dense layer with arguments, num of features, we want to find probability distributions of our characters. And other argument is mentioning activation to softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "iZKl0LtdvBNy",
    "outputId": "34a3c344-5985-4e3b-e1ac-392c18739308"
   },
   "source": [
    "Then usual, compiling the model. Stating the loss function, to 'categorical crossentropy' because we have a mulitclassification problem where we have one hot encoded dara. Setting optimizer to 'adam'. And will use 'accuracy' as training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 128)               18304     \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 5, 128)            32896     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 5, 14)             1806      \n",
      "=================================================================\n",
      "Total params: 53,006\n",
      "Trainable params: 53,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_units = 128\n",
    "max_time_steps = 5\n",
    "\n",
    "model = Sequential([\n",
    "    SimpleRNN(hidden_units, input_shape=(None, num_of_features)),\n",
    "    RepeatVector(max_time_steps),\n",
    "    SimpleRNN(hidden_units, return_sequences=True),\n",
    "    TimeDistributed(Dense(num_of_features, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vectorize and De-Vectorize Data:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the model we want to train, also have the data or at least a way to create data but it's not in the desired format we want. We want to vectorize the string data, so that it can be used with the RNN model we made above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a funtion to vectorize the example and label pair generated via above generate data fucntion. \n",
    "Funtion will take example and label as its aruments. Then creating placeholdeer for example and label, having shape as (max time steps, no. of features).\n",
    "Then store the difference between max time steps and example, and max time steps and label.\n",
    "Essentially, what we doing is one hot encoding it just loop thorugh all the characters and save the character to index representation in the placeholders we created.\n",
    "And we seperately loop through it if there's padding at the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to check whether it worked properly we need to de-vectorized the examples and label back to string. Although its not just to verify, we only need to vectorized version for the model. Hence, we need convert some test examples back to human readable format so that we can read and verify the results.\n",
    "Funtion would take argument as example or lable, doesn't we need doing this to verify only. \n",
    "We use index to character dictionary to convert indices back into character. And we will be argmax funtion to get the maximum value which in this case is 1. And enumerate through indices can vectors and this will give a string of characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ci29YaA6vBN1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15+2 17\n",
      "(5, 14) (5, 14)\n"
     ]
    }
   ],
   "source": [
    "def vectorize_example(exp, label):\n",
    "    x = np.zeros((max_time_steps, num_of_features))\n",
    "    y = np.zeros((max_time_steps, num_of_features))\n",
    "    \n",
    "    diff_x = max_time_steps - len(exp)\n",
    "    diff_y = max_time_steps - len(label)\n",
    "    \n",
    "    for i, c in enumerate(exp):\n",
    "        x[i + diff_x, char_to_index[c]] = 1\n",
    "    for i in range(diff_x):\n",
    "        x[i, char_to_index['0']] = 1\n",
    "    for i, c in enumerate(label):\n",
    "        y[i + diff_y, char_to_index[c]] = 1\n",
    "    for i in range(diff_y):\n",
    "        y[i, char_to_index['0']] = 1\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "e, l = generate_data()\n",
    "print(e, l)\n",
    "x, y = vectorize_example(e, l)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We not printing x and y because they just be two arrays, with 0's on most the places accept where there's our character represented by will 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_nARKq-bvBN9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'015+2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def devectorize_example(exp):\n",
    "    result = [index_to_char[np.argmax(vec)] for i, vec in enumerate(exp)]\n",
    "    return ''.join(result)\n",
    "\n",
    "devectorize_example(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KCGUxNwNvBOA",
    "outputId": "a721451a-7f7c-42be-bdf0-0c96381f2eb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00017'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devectorize_example(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Dataset:__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "T5rdrhiVvBOI",
    "outputId": "426b3722-78cf-494f-8bbf-1cdf7eab3113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 5, 14) (2000, 5, 14)\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(num_examples=2000):\n",
    "    x = np.zeros((num_examples, max_time_steps, num_of_features))\n",
    "    y = np.zeros((num_examples, max_time_steps, num_of_features))\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        e, l = generate_data()\n",
    "        e_v, l_v = vectorize_example(e, l)\n",
    "        \n",
    "        x[i] = e_v\n",
    "        y[i] = l_v\n",
    "        \n",
    "    return x, y\n",
    "\n",
    "x, y = create_dataset()\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RRwiuwcQvBOL",
    "outputId": "07591064-5c57-4adb-ccf6-c226457d2263"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'73+14'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devectorize_example(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cRUvtd0IvBOO",
    "outputId": "d07ce279-06ba-44c4-de91-0ec4fb551877"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00087'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devectorize_example(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training the Model:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we need some callbacks. One to simplfy the logging and we possibly going to train the model for 100s of epochs so need too keep log of validation accuracy. Its a pretty lenghty task so, we use lambda function to print out only the validation accuracy. \n",
    "Second callback is an early stopping callback by monitoring validation loss and set patience as 10. \n",
    "And then we simply fit the model.\n",
    "And accuracy we will get will be character level not the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2686
    },
    "colab_type": "code",
    "id": "S8HWRdiOvBOS",
    "outputId": "2365b11f-30e8-48cc-e940-4d3267771fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88 _ 0.87 _ 0.89 _ 0.88 _ 0.92 _ 0.93 _ 0.95 _ 0.95 _ 0.96 _ 0.96 _ 0.96 _ 0.97 _ 0.96 _ 0.97 _ 0.97 _ 0.97 _ 0.97 _ 0.97 _ 0.97 _ 0.97 _ 0.98 _ 0.98 _ 0.98 _ 0.97 _ 0.97 _ 0.97 _ 0.97 _ 0.97 _ 0.98 _ 0.97 _ 0.98 _ 0.98 _ 0.98 _ 0.97 _ 0.97 _ 0.98 _ 0.98 _ 0.98 _ 0.97 _ 0.98 _ 0.98 _ 0.97 _ 0.98 _ 0.98 _ 0.98 _ 0.97 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.97 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.97 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ 0.98 _ "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14611bc8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_cb = LambdaCallback(\n",
    "    on_epoch_end = lambda e, l: print('{:.2f}'.format(l['val_accuracy']), end = ' _ ')\n",
    ")\n",
    "\n",
    "es_cb = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(x, y, epochs=500, batch_size=256, validation_split=0.2, \n",
    "         verbose=False, callbacks=[es_cb,l_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "tlNtu_9ZvBOY",
    "outputId": "82198555-f0e8-4a7a-9125-cc61cf4f4a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mInput: 62+33 Out: 00095 Pred: 00095\u001b[0m\n",
      "\u001b[32mInput: 85+54 Out: 00139 Pred: 00139\u001b[0m\n",
      "\u001b[32mInput: 53+50 Out: 00103 Pred: 00103\u001b[0m\n",
      "\u001b[32mInput: 38+84 Out: 00122 Pred: 00122\u001b[0m\n",
      "\u001b[32mInput: 25+81 Out: 00106 Pred: 00106\u001b[0m\n",
      "\u001b[32mInput: 80+80 Out: 00160 Pred: 00160\u001b[0m\n",
      "\u001b[31mInput: 77+22 Out: 00099 Pred: 00009\u001b[0m\n",
      "\u001b[32mInput: 70+66 Out: 00136 Pred: 00136\u001b[0m\n",
      "\u001b[32mInput: 37+92 Out: 00129 Pred: 00129\u001b[0m\n",
      "\u001b[32mInput: 07+11 Out: 00018 Pred: 00018\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = create_dataset(10)\n",
    "preds = model.predict(x_test)\n",
    "\n",
    "for i , pred in enumerate(preds):\n",
    "    y = devectorize_example(y_test[i])\n",
    "    y_hat = devectorize_example(pred)\n",
    "    if y == y_hat:\n",
    "        col = 'green'\n",
    "    else: \n",
    "        y != y_hat\n",
    "        col = 'red'\n",
    "        \n",
    "    out = 'Input: ' + devectorize_example(x_test[i]) +  ' Out: ' + y + ' Pred: ' + y_hat\n",
    "    \n",
    "    print(colored(out, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "addition_simpleRNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
